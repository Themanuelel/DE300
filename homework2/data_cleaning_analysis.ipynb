{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3241154-6031-42a9-9623-01c719f3dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/1204852381.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['painloc'] = df_subset['painloc'].fillna(df_subset['painloc'].mode()[0])\n",
      "/tmp/ipykernel_124/1204852381.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['painexer'] = df_subset['painexer'].fillna(df_subset['painexer'].mode()[0])\n",
      "/tmp/ipykernel_124/1204852381.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['thaldur'].fillna(mean_thaldur, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['thalach'].fillna(mean_thalach, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['fbs'].fillna(mode_fbs, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['prop'].fillna(mode_prop, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['nitr'].fillna(mode_nitr, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['pro'].fillna(mode_pro, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['diuretic'].fillna(mode_diuretic, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['exang'].fillna(mode_exang, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['slope'].fillna(mode_slope, inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subs[col].fillna(df_subs[col].median(), inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subs[col].fillna(round(df_subs[col].mean(), 1), inplace=True)\n",
      "/tmp/ipykernel_124/1204852381.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset[subs_cols] = df_subs\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(\"heart_disease.csv\")\n",
    "\n",
    "df = df[0:899]\n",
    "\n",
    "# Retain only the specified columns\n",
    "columns_to_keep = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', \n",
    "                   'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', \n",
    "                   'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "\n",
    "df_subset = df[columns_to_keep]\n",
    "\n",
    "## Replaces all values for painloc and painexer with the mode value for those columns\n",
    "df_subset['painloc'] = df_subset['painloc'].fillna(df_subset['painloc'].mode()[0])\n",
    "df_subset['painexer'] = df_subset['painexer'].fillna(df_subset['painexer'].mode()[0])\n",
    "\n",
    "## Values < 100 are replaced with 100\n",
    "df_subset.loc[df_subset['trestbps'] < 100, 'trestbps'] = 100\n",
    "\n",
    "## Values less than 0 are replaced with 0, those greater than 4 are replaced with 4\n",
    "df_subset.loc[df_subset['oldpeak'] < 0, 'oldpeak'] = 0\n",
    "df_subset.loc[df_subset['oldpeak'] > 4, 'oldpeak'] = 4\n",
    "\n",
    "## Filling missing values with the mean\n",
    "mean_thaldur = round(df_subset['thaldur'].mean(), 1)\n",
    "mean_thalach = round(df_subset['thalach'].mean(), 1)\n",
    "\n",
    "df_subset['thaldur'].fillna(mean_thaldur, inplace=True)\n",
    "df_subset['thalach'].fillna(mean_thalach, inplace=True)\n",
    "\n",
    "## Filling missing values with the mode value\n",
    "mode_fbs = df_subset['fbs'].mode()[0]\n",
    "mode_prop = df_subset['prop'].mode()[0]\n",
    "mode_nitr = df_subset['nitr'].mode()[0]\n",
    "mode_pro = df_subset['pro'].mode()[0]\n",
    "mode_diuretic = df_subset['diuretic'].mode()[0]\n",
    "mode_exang = df_subset['exang'].mode()[0]\n",
    "mode_slope = df_subset['slope'].mode()[0]\n",
    "\n",
    "df_subset['fbs'].fillna(mode_fbs, inplace=True)\n",
    "df_subset['prop'].fillna(mode_prop, inplace=True)\n",
    "df_subset['nitr'].fillna(mode_nitr, inplace=True)\n",
    "df_subset['pro'].fillna(mode_pro, inplace=True)\n",
    "df_subset['diuretic'].fillna(mode_diuretic, inplace=True)\n",
    "df_subset['exang'].fillna(mode_exang, inplace=True)\n",
    "df_subset['slope'].fillna(mode_slope, inplace=True)\n",
    "\n",
    "## Also replaces values greater than 1 with the mode for that column\n",
    "df_subset.loc[df_subset['fbs'] > 1, 'fbs'] = mode_fbs\n",
    "df_subset.loc[df_subset['prop'] > 1, 'prop'] = mode_prop\n",
    "df_subset.loc[df_subset['nitr'] > 1, 'nitr'] = mode_nitr\n",
    "df_subset.loc[df_subset['pro'] > 1, 'pro'] = mode_pro\n",
    "df_subset.loc[df_subset['diuretic'] > 1, 'diuretic'] = mode_diuretic\n",
    "\n",
    "## These columns are checked for skewness. If they appear to be skewed, the missing values are filled with\n",
    "## the median. If not skewed, the missing values are filled with the mean. \n",
    "subs_cols = ['trestbps', 'oldpeak', 'thaldur', 'thalach']\n",
    "df_subs = df_subset[subs_cols]\n",
    "\n",
    "skewness = df_subs.skew()\n",
    "\n",
    "for col in df_subs.columns:\n",
    "    if abs(skewness[col]) < 0.5:\n",
    "        # If not skewed, replace missing values with mean\n",
    "        df_subs[col].fillna(round(df_subs[col].mean(), 1), inplace=True)\n",
    "    else:\n",
    "        # If skewed, replace missing values with median\n",
    "        df_subs[col].fillna(df_subs[col].median(), inplace=True)\n",
    "\n",
    "df_subset[subs_cols] = df_subs\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df_subset.to_csv(\"heart_disease_subset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "892984cf-79b0-41ff-ba53-63ad8269bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(899, 56)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ab66d9-6af0-47de-933b-d06dfc70aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached Scrapy-2.11.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting Twisted>=18.9.0 (from scrapy)\n",
      "  Using cached twisted-24.3.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.11/site-packages (from scrapy) (41.0.4)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Using cached itemloaders-1.2.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Using cached parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from scrapy) (23.2.0)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Using cached queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Using cached service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Using cached w3lib-2.1.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Using cached zope.interface-6.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (42 kB)\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Using cached Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Using cached itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from scrapy) (68.2.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from scrapy) (23.2)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Using cached tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting lxml>=4.4.1 (from scrapy)\n",
      "  Using cached lxml-5.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Using cached PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.11/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.11/site-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Using cached Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Using cached constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=22.10.0 (from Twisted>=18.9.0->scrapy)\n",
      "  Using cached incremental-22.10.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from Twisted>=18.9.0->scrapy) (4.8.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from tldextract->scrapy) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from tldextract->scrapy) (2.31.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Using cached requests_file-2.0.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Using cached filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n",
      "Using cached Scrapy-2.11.1-py2.py3-none-any.whl (287 kB)\n",
      "Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached itemloaders-1.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached lxml-5.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "Using cached parsel-1.9.1-py2.py3-none-any.whl (17 kB)\n",
      "Using cached Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Using cached PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Using cached queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Using cached service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
      "Using cached twisted-24.3.0-py3-none-any.whl (3.2 MB)\n",
      "Using cached w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached zope.interface-6.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (249 kB)\n",
      "Using cached tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
      "Using cached Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Using cached constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Using cached filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Using cached incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, constantly, automat, Twisted, requests-file, pyasn1-modules, parsel, tldextract, service-identity, itemloaders, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.3.0 automat-22.10.0 constantly-23.10.4 cssselect-1.2.0 filelock-3.14.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.9.0 itemloaders-1.2.0 jmespath-1.0.1 lxml-5.2.1 parsel-1.9.1 protego-0.3.1 pyasn1-0.6.0 pyasn1-modules-0.4.0 queuelib-1.7.0 requests-file-2.0.0 scrapy-2.11.1 service-identity-24.1.0 tldextract-5.1.2 w3lib-2.1.2 zope.interface-6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33d7407-ad3f-4e13-b7dd-095c9329ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15–17 1.6\n",
      "18–24 7.3\n",
      "25–34 10.9\n",
      "35–44 10.9\n",
      "45–54 13.8\n",
      "55–64 14.9\n",
      "65–74 8.7\n",
      "75 years and over 2.9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "DATA_FOLDER = Path('data/')\n",
    "URL = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release'\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "def get_selector_from_url(url:str) -> Selector:\n",
    "    response = requests.get(url)\n",
    "    return Selector(text=response.content)\n",
    "\n",
    "def parse_row(row:Selector) -> List[str]:\n",
    "    '''\n",
    "    Parses a html row into a list of individual elements\n",
    "    '''\n",
    "    cells = row.xpath('.//th | .//td')\n",
    "    row_data = []\n",
    "    \n",
    "    for cell in cells:\n",
    "        cell_text = cell.xpath('normalize-space(.)').get()\n",
    "        cell_text = re.sub(r'<.*?>', ' ', cell_text)  # Remove remaining HTML tags\n",
    "        # if there are br tags, there will be some binary characters\n",
    "        cell_text = cell_text.replace('\\xa0', '')  # Remove \\xa0 characters\n",
    "        row_data.append(cell_text)\n",
    "    \n",
    "    return row_data\n",
    "\n",
    "def parse_table_as_df(table_sel:Selector,header:bool=True) -> pd.DataFrame:\n",
    "    '''\n",
    "    Parses a html table and returns it as a Pandas DataFrame\n",
    "    '''\n",
    "    # extract rows\n",
    "    rows = table_sel.xpath('./tbody//tr')\n",
    "    \n",
    "    # parse header and the remaining rows\n",
    "    columns = None\n",
    "    start_row = 0\n",
    "    if header:\n",
    "        columns = parse_row(rows[0])\n",
    "        \n",
    "    table_data = [parse_row(row) for row in rows[start_row:]]\n",
    "    \n",
    "    # return data frame\n",
    "    return pd.DataFrame(table_data,columns=columns)\n",
    "\n",
    "selector = get_selector_from_url(URL)\n",
    "\n",
    "# select the table containing smoking data by age\n",
    "smoking_table = selector.xpath('//table[caption[contains(text(),\"Proportion of people 15 years and over who were current daily smokers by age\")]]')\n",
    "\n",
    "if smoking_table:\n",
    "    try:\n",
    "        df = parse_table_as_df(smoking_table[0], header=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    else:\n",
    "        \n",
    "        # Print the data rows\n",
    "        for index, row in df.iterrows():\n",
    "            print(row.iloc[0], row.iloc[10])\n",
    "else:\n",
    "    print(\"Smoking table not found on the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86079a4d-8048-46d6-bd4b-3fd72a6219f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 13 of every 100 adult men (13.1%)\n",
      "About 10 of every 100 adult women (10.1%)\n",
      "About 5 of every 100 adults aged 18–24 years (5.3%)\n",
      "Nearly 13 of every 100 adults aged 25–44 years (12.6%)\n",
      "Nearly 15 of every 100 adults aged 45–64 years (14.9%)\n",
      "About 8 of every 100 adults aged 65 years and older (8.3%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_FOLDER = Path('data/')\n",
    "URL = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "def get_selector_from_url(url:str) -> Selector:\n",
    "    response = requests.get(url)\n",
    "    return Selector(text=response.content)\n",
    "\n",
    "selector = get_selector_from_url(URL)\n",
    "\n",
    "# Select the paragraphs containing the desired text\n",
    "paragraphsM = selector.xpath(\"//li[contains(text(), 'adult men')]/text()\").get()\n",
    "paragraphsF = selector.xpath(\"//li[contains(text(), 'adult women')]/text()\").get()\n",
    "\n",
    "print(paragraphsM.strip())\n",
    "print(paragraphsF.strip())\n",
    "\n",
    "uls = selector.xpath(\"//div[h4[contains(text(), 'By Age')]]/following-sibling::div//ul/li/text()\").getall()\n",
    "\n",
    "for line in uls:\n",
    "    print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdc53cd5-f358-45bb-9f28-d26842b7a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/1187820860.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['age'] = df_subset['age'].astype(int)\n",
      "/tmp/ipykernel_124/1187820860.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['smoking_src1'] = df_subset.apply(lambda row: row['smoke'] if row['smoke'] in [0, 1] else get_smoking_percentage(row['age']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Source 1 smoke column imputation\n",
    "\n",
    "def get_smoking_percentage(age: int) -> float:\n",
    "    # Define the smoking percentage based on the age range\n",
    "    if 15 <= age <= 17:\n",
    "        return .016\n",
    "    elif 18 <= age <= 24:\n",
    "        return .073\n",
    "    elif 25 <= age <= 34:\n",
    "        return .109\n",
    "    elif 35 <= age <= 44:\n",
    "        return .109\n",
    "    elif 45 <= age <= 54:\n",
    "        return .138\n",
    "    elif 55 <= age <= 64:\n",
    "        return .149\n",
    "    elif 65 <= age <= 74:\n",
    "        return .087\n",
    "    elif age >= 75:\n",
    "        return .029\n",
    "    else:\n",
    "        return None  # Return None for unknown age ranges\n",
    "\n",
    "\n",
    "# Convert the 'age' column to integer type\n",
    "df_subset['age'] = df_subset['age'].astype(int)\n",
    "\n",
    "# Apply the function to create a new column 'smoking_src1' with the updated values\n",
    "df_subset['smoking_src1'] = df_subset.apply(lambda row: row['smoke'] if row['smoke'] in [0, 1] else get_smoking_percentage(row['age']), axis=1)\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file, overwriting the existing file\n",
    "df_subset.to_csv('heart_disease_subset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd22706-b165-4cbe-aa89-967bbfecef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/4277454062.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['sex'] = df_subset['sex'].astype(int)\n",
      "/tmp/ipykernel_124/4277454062.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset['smoke_src2'] = df_subset.apply(lambda row: row['smoke'] if row['smoke'] in [0, 1] else get_smoking_percentage(row['age'], row['sex']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Source 2 smoke column imputation\n",
    "def get_smoking_percentage(age: int, sex: int) -> float:\n",
    "    if sex == 0:  # Female\n",
    "        if 18 <= age <= 24:\n",
    "            return .053\n",
    "        elif 25 <= age <= 44:\n",
    "            return .126\n",
    "        elif 45 <= age <= 64:\n",
    "            return .149\n",
    "        elif age >= 65:\n",
    "            return .083\n",
    "        \n",
    "    elif sex == 1:\n",
    "        if 18 <= age <= 24:\n",
    "            return round(.053 * (.131 / .101), 3)\n",
    "        elif 25 <= age <= 44:\n",
    "            return round(.126 * (.131 / .101), 3)\n",
    "        elif 45 <= age <= 64:\n",
    "            return round(.149 * (.131 / .101), 3)\n",
    "        elif age >= 65:\n",
    "            return round(.083 * (.131 / .101), 3)\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "df_subset['sex'] = df_subset['sex'].astype(int)\n",
    "\n",
    "# Apply the function to create a new column 'smoke_src2' with the updated values\n",
    "df_subset['smoke_src2'] = df_subset.apply(lambda row: row['smoke'] if row['smoke'] in [0, 1] else get_smoking_percentage(row['age'], row['sex']), axis=1)\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file, overwriting the existing file\n",
    "df_subset.to_csv('heart_disease_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9d4a6f4-8618-4700-bd59-5d773837fe23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124/2984597335.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_subset.drop(columns=['smoke'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Remove the 'smoke' column\n",
    "df_subset.drop(columns=['smoke'], inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file, overwriting the existing file\n",
    "df_subset.to_csv('heart_disease_subset.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264e93d9-1842-4988-9eac-9aacbce7c73b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 1: Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf_subset\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Features\u001b[39;00m\n\u001b[1;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m df_subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Target variable\u001b[39;00m\n\u001b[1;32m     12\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, stratify\u001b[38;5;241m=\u001b[39my, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_subset' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Step 1: Split the data into training and test sets\n",
    "X = df_subset.drop(columns=['target'])  # Features\n",
    "y = df_subset['target']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=42)\n",
    "\n",
    "# Step 2: Train binary classification models\n",
    "# Example models: Logistic Regression, Random Forest, and Support Vector Machine (SVM)\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "random_forest_model = RandomForestClassifier(max_depth=100)\n",
    "svm_model = SVC(C = 1000)\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Evaluate the performance of the models on the test data\n",
    "logistic_pred = logistic_model.predict(X_test)\n",
    "random_forest_pred = random_forest_model.predict(X_test)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_pred)\n",
    "random_forest_accuracy = accuracy_score(y_test, random_forest_pred)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "\n",
    "# Classification report\n",
    "logistic_report = classification_report(y_test, logistic_pred)\n",
    "random_forest_report = classification_report(y_test, random_forest_pred)\n",
    "svm_report = classification_report(y_test, svm_pred)\n",
    "\n",
    "# Perform 5-fold cross-validation on logistic regression model\n",
    "logistic_cv_scores = cross_val_score(logistic_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Perform 5-fold cross-validation on random forest model\n",
    "random_forest_cv_scores = cross_val_score(random_forest_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Perform 5-fold cross-validation on SVM model\n",
    "svm_cv_scores = cross_val_score(svm_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Calculate mean cross-validation scores\n",
    "logistic_mean_cv_score = logistic_cv_scores.mean()\n",
    "random_forest_mean_cv_score = random_forest_cv_scores.mean()\n",
    "svm_mean_cv_score = svm_cv_scores.mean()\n",
    "\n",
    "# Print results\n",
    "print(\"Logistic Regression Model Accuracy:\", logistic_accuracy)\n",
    "print(\"Logistic Regression Model Classification Report:\\n\", logistic_report)\n",
    "\n",
    "print(\"\\nRandom Forest Model Accuracy:\", random_forest_accuracy)\n",
    "print(\"Random Forest Model Classification Report:\\n\", random_forest_report)\n",
    "\n",
    "print(\"\\nSVM Model Accuracy:\", svm_accuracy)\n",
    "print(\"SVM Model Classification Report:\\n\", svm_report)\n",
    "\n",
    "# Print mean cross-validation scores\n",
    "print(\"Logistic Regression Model Mean Cross-Validation Score:\", logistic_mean_cv_score)\n",
    "print(\"Random Forest Model Mean Cross-Validation Score:\", random_forest_mean_cv_score)\n",
    "print(\"SVM Model Mean Cross-Validation Score:\", svm_mean_cv_score)\n",
    "\n",
    "\n",
    "\n",
    "### ANALYSIS\n",
    "# Based on the results, I would choose the random forest model. Although all three models have similar scores for accuracy, precision, recall, and f1-score, the cross-validation analysis shows that the random forest model is slightly better suited to unseen data compared to the other models. While its accuracy is slightly less than the other models, I would trust the random forest model over the others when it comes to testing on new data.\n",
    "\n",
    "\n",
    "# Running this code block has made my instance freeze many times. This may be due to the resource allocation for this t2 instance. If it fails to run, this is what the output looks like when run on my local pc:\n",
    "\n",
    "#Logistic Regression Model Accuracy: 0.7444444444444445\n",
    "#Logistic Regression Model Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "#\n",
    "#         0.0       0.72      0.70      0.71        40\n",
    "#         1.0       0.76      0.78      0.77        50\n",
    "#\n",
    "#    accuracy                           0.74        90\n",
    "#   macro avg       0.74      0.74      0.74        90\n",
    "#weighted avg       0.74      0.74      0.74        90\n",
    "#\n",
    "#\n",
    "#Random Forest Model Accuracy: 0.7333333333333333\n",
    "#Random Forest Model Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "#\n",
    "#         0.0       0.70      0.70      0.70        40\n",
    "#         1.0       0.76      0.76      0.76        50\n",
    "#\n",
    "#    accuracy                           0.73        90\n",
    "#   macro avg       0.73      0.73      0.73        90\n",
    "#weighted avg       0.73      0.73      0.73        90\n",
    "#\n",
    "#\n",
    "#SVM Model Accuracy: 0.7333333333333333\n",
    "#SVM Model Classification Report:\n",
    "#               precision    recall  f1-score   support\n",
    "#\n",
    "#         0.0       0.72      0.65      0.68        40\n",
    "#         1.0       0.74      0.80      0.77        50\n",
    "#\n",
    "#    accuracy                           0.73        90\n",
    "#   macro avg       0.73      0.73      0.73        90\n",
    "#weighted avg       0.73      0.73      0.73        90\n",
    "#\n",
    "#Logistic Regression Model Mean Cross-Validation Score: 0.8059658001686987\n",
    "#Random Forest Model Mean Cross-Validation Score: 0.8109347442680775\n",
    "#SVM Model Mean Cross-Validation Score: 0.8034813281190093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a1b4-6b94-472a-bba4-d37f0ed916b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
