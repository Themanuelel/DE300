{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3241154-6031-42a9-9623-01c719f3dac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/24 16:46:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean, median\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Heart Disease Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame\n",
    "df = spark.read.csv(\"heart_disease.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df = df.limit(899)\n",
    "\n",
    "# Retain only the specified columns\n",
    "columns_to_keep = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', \n",
    "                   'smoke', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', \n",
    "                   'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "\n",
    "df_subset = df.select(columns_to_keep)\n",
    "\n",
    "## Replaces all values for painloc and painexer with the mode value for those columns\n",
    "painloc_mode = df_subset.groupby().agg({\"painloc\": \"max\"}).collect()[0][0]\n",
    "painexer_mode = df_subset.groupby().agg({\"painexer\": \"max\"}).collect()[0][0]\n",
    "\n",
    "df_subset = df_subset.fillna({\"painloc\": painloc_mode, \"painexer\": painexer_mode})\n",
    "\n",
    "## Values < 100 are replaced with 100\n",
    "df_subset = df_subset.withColumn(\"trestbps\", when(col(\"trestbps\") < 100, 100).otherwise(col(\"trestbps\")))\n",
    "\n",
    "## Values less than 0 are replaced with 0, those greater than 4 are replaced with 4\n",
    "df_subset = df_subset.withColumn(\"oldpeak\", when(col(\"oldpeak\") < 0, 0).otherwise(col(\"oldpeak\")))\n",
    "df_subset = df_subset.withColumn(\"oldpeak\", when(col(\"oldpeak\") > 4, 4).otherwise(col(\"oldpeak\")))\n",
    "\n",
    "## Filling missing values with the mean\n",
    "mean_thaldur = df_subset.select(mean(\"thaldur\")).collect()[0][0]\n",
    "mean_thalach = df_subset.select(mean(\"thalach\")).collect()[0][0]\n",
    "\n",
    "df_subset = df_subset.fillna({\"thaldur\": mean_thaldur, \"thalach\": mean_thalach})\n",
    "\n",
    "## Filling missing values with the mode value\n",
    "mode_fbs = df_subset.groupby().agg({\"fbs\": \"max\"}).collect()[0][0]\n",
    "mode_prop = df_subset.groupby().agg({\"prop\": \"max\"}).collect()[0][0]\n",
    "mode_nitr = df_subset.groupby().agg({\"nitr\": \"max\"}).collect()[0][0]\n",
    "mode_pro = df_subset.groupby().agg({\"pro\": \"max\"}).collect()[0][0]\n",
    "mode_diuretic = df_subset.groupby().agg({\"diuretic\": \"max\"}).collect()[0][0]\n",
    "mode_exang = df_subset.groupby().agg({\"exang\": \"max\"}).collect()[0][0]\n",
    "mode_slope = df_subset.groupby().agg({\"slope\": \"max\"}).collect()[0][0]\n",
    "\n",
    "df_subset = df_subset.fillna({\"fbs\": mode_fbs, \"prop\": mode_prop, \"nitr\": mode_nitr, \n",
    "                               \"pro\": mode_pro, \"diuretic\": mode_diuretic, \n",
    "                               \"exang\": mode_exang, \"slope\": mode_slope})\n",
    "\n",
    "## Also replaces values greater than 1 with the mode for that column\n",
    "df_subset = df_subset.withColumn(\"fbs\", when(col(\"fbs\") > 1, mode_fbs).otherwise(col(\"fbs\")))\n",
    "df_subset = df_subset.withColumn(\"prop\", when(col(\"prop\") > 1, mode_prop).otherwise(col(\"prop\")))\n",
    "df_subset = df_subset.withColumn(\"nitr\", when(col(\"nitr\") > 1, mode_nitr).otherwise(col(\"nitr\")))\n",
    "df_subset = df_subset.withColumn(\"pro\", when(col(\"pro\") > 1, mode_pro).otherwise(col(\"pro\")))\n",
    "df_subset = df_subset.withColumn(\"diuretic\", when(col(\"diuretic\") > 1, mode_diuretic).otherwise(col(\"diuretic\")))\n",
    "\n",
    "## These columns are checked for skewness. If they appear to be skewed, the missing values are filled with\n",
    "## the median. If not skewed, the missing values are filled with the mean. \n",
    "subs_cols = ['trestbps', 'oldpeak', 'thaldur', 'thalach']\n",
    "\n",
    "skewness = {col_name: df_subset.agg({col_name: \"skewness\"}).collect()[0][0] for col_name in subs_cols}\n",
    "\n",
    "for col_name, col_skewness in skewness.items():\n",
    "    if abs(col_skewness) < 0.5:\n",
    "        # If not skewed, replace missing values with mean\n",
    "        mean_val = df_subset.select(mean(col_name)).collect()[0][0]\n",
    "        df_subset = df_subset.fillna({col_name: mean_val})\n",
    "    else:\n",
    "        # If skewed, replace missing values with median\n",
    "        median_val = df_subset.approxQuantile(col_name, [0.5], 0.25)[0]\n",
    "        df_subset = df_subset.fillna({col_name: median_val})\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df_subset.write.mode(\"overwrite\").csv(\"heart_disease_subset.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892984cf-79b0-41ff-ba53-63ad8269bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ab66d9-6af0-47de-933b-d06dfc70aea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /tmp/demos/lib/python3.10/site-packages (2.11.2)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (24.1.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (2.1.2)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (6.4.post1)\n",
      "Requirement already satisfied: lxml>=4.4.1 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (5.2.2)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (24.3.0)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (1.9.1)\n",
      "Requirement already satisfied: protego>=0.1.15 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (0.3.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (42.0.7)\n",
      "Requirement already satisfied: tldextract in /tmp/demos/lib/python3.10/site-packages (from scrapy) (5.1.2)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /tmp/demos/lib/python3.10/site-packages (from scrapy) (59.6.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /tmp/demos/lib/python3.10/site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: packaging in /tmp/demos/lib/python3.10/site-packages (from scrapy) (24.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /tmp/demos/lib/python3.10/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /tmp/demos/lib/python3.10/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: pyasn1-modules in /tmp/demos/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (0.4.0)\n",
      "Requirement already satisfied: pyasn1 in /tmp/demos/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (0.6.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /tmp/demos/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
      "Requirement already satisfied: automat>=0.8.0 in /tmp/demos/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /tmp/demos/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /tmp/demos/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (4.11.0)\n",
      "Requirement already satisfied: constantly>=15.1 in /tmp/demos/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: incremental>=22.10.0 in /tmp/demos/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (22.10.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /tmp/demos/lib/python3.10/site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /tmp/demos/lib/python3.10/site-packages (from tldextract->scrapy) (3.14.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in /tmp/demos/lib/python3.10/site-packages (from tldextract->scrapy) (2.32.2)\n",
      "Requirement already satisfied: idna in /tmp/demos/lib/python3.10/site-packages (from tldextract->scrapy) (3.7)\n",
      "Requirement already satisfied: six in /tmp/demos/lib/python3.10/site-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /tmp/demos/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /tmp/demos/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /tmp/demos/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /tmp/demos/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33d7407-ad3f-4e13-b7dd-095c9329ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+\n",
      "|15–17            |1.6 |\n",
      "+-----------------+----+\n",
      "|18–24            |7.3 |\n",
      "|25–34            |10.9|\n",
      "|35–44            |10.9|\n",
      "|45–54            |13.8|\n",
      "|55–64            |14.9|\n",
      "|65–74            |8.7 |\n",
      "|75 years and over|2.9 |\n",
      "+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "from pathlib import Path\n",
    "import re\n",
    "from typing import List\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Constants\n",
    "DATA_FOLDER = Path('data/')\n",
    "URL = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release'\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "def get_selector_from_url(url: str) -> Selector:\n",
    "    response = requests.get(url)\n",
    "    return Selector(text=response.content)\n",
    "\n",
    "def parse_row(row: Selector) -> List[str]:\n",
    "    '''\n",
    "    Parses an HTML row into a list of individual elements\n",
    "    '''\n",
    "    cells = row.xpath('.//th | .//td')\n",
    "    row_data = []\n",
    "    \n",
    "    for cell in cells:\n",
    "        cell_text = cell.xpath('normalize-space(.)').get()\n",
    "        cell_text = re.sub(r'<.*?>', ' ', cell_text)  # Remove remaining HTML tags\n",
    "        cell_text = cell_text.replace('\\xa0', '')  # Remove \\xa0 characters\n",
    "        row_data.append(cell_text)\n",
    "    \n",
    "    return row_data\n",
    "\n",
    "def parse_table_as_list(table_sel: Selector, header: bool = True) -> (List[str], List[List[str]]):\n",
    "    '''\n",
    "    Parses an HTML table and returns it as a list of lists\n",
    "    '''\n",
    "    # Extract rows\n",
    "    rows = table_sel.xpath('./tbody//tr')\n",
    "    \n",
    "    # Parse header and the remaining rows\n",
    "    columns = None\n",
    "    if header:\n",
    "        columns = parse_row(rows[0])\n",
    "        rows = rows[1:]  # Skip the header row\n",
    "    \n",
    "    table_data = [parse_row(row) for row in rows]\n",
    "    \n",
    "    return columns, table_data\n",
    "\n",
    "selector = get_selector_from_url(URL)\n",
    "\n",
    "# Select the table containing smoking data by age\n",
    "smoking_table = selector.xpath('//table[caption[contains(text(),\"Proportion of people 15 years and over who were current daily smokers by age\")]]')\n",
    "\n",
    "if smoking_table:\n",
    "    try:\n",
    "        columns, table_data = parse_table_as_list(smoking_table[0], header=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    else:\n",
    "        # Create a Spark DataFrame from the parsed data\n",
    "        spark_df = spark.createDataFrame(table_data, schema=columns)\n",
    "        \n",
    "        # Select only the first and specific column (11th column in this case, considering zero-based indexing)\n",
    "        # Adjust the column index if the desired column is different\n",
    "        selected_df = spark_df.select(col(columns[0]), col(f\"`{columns[10]}`\"))\n",
    "        \n",
    "        # Show the selected DataFrame\n",
    "        selected_df.show(truncate=False)\n",
    "else:\n",
    "    print(\"Smoking table not found on the webpage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86079a4d-8048-46d6-bd4b-3fd72a6219f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About 13 of every 100 adult men (13.1%)\n",
      "About 10 of every 100 adult women (10.1%)\n",
      "About 5 of every 100 adults aged 18–24 years (5.3%)\n",
      "Nearly 13 of every 100 adults aged 25–44 years (12.6%)\n",
      "Nearly 15 of every 100 adults aged 45–64 years (14.9%)\n",
      "About 8 of every 100 adults aged 65 years and older (8.3%)\n",
      "+-----------+----------------------------------------------------------+\n",
      "|Category   |Text                                                      |\n",
      "+-----------+----------------------------------------------------------+\n",
      "|adult men  |About 13 of every 100 adult men (13.1%)                   |\n",
      "|adult women|About 10 of every 100 adult women (10.1%)                 |\n",
      "|By Age     |About 5 of every 100 adults aged 18–24 years (5.3%)       |\n",
      "|By Age     |Nearly 13 of every 100 adults aged 25–44 years (12.6%)    |\n",
      "|By Age     |Nearly 15 of every 100 adults aged 45–64 years (14.9%)    |\n",
      "|By Age     |About 8 of every 100 adults aged 65 years and older (8.3%)|\n",
      "+-----------+----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Constants\n",
    "URL = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
    "\n",
    "def get_selector_from_url(url: str) -> Selector:\n",
    "    response = requests.get(url)\n",
    "    return Selector(text=response.content)\n",
    "\n",
    "# Fetch and parse the HTML content\n",
    "selector = get_selector_from_url(URL)\n",
    "\n",
    "# Extract the paragraphs containing the desired text\n",
    "paragraphsM = selector.xpath(\"//li[contains(text(), 'adult men')]/text()\").get()\n",
    "paragraphsF = selector.xpath(\"//li[contains(text(), 'adult women')]/text()\").get()\n",
    "\n",
    "# Print extracted paragraphs\n",
    "if paragraphsM:\n",
    "    print(paragraphsM.strip())\n",
    "if paragraphsF:\n",
    "    print(paragraphsF.strip())\n",
    "\n",
    "# Extract list items following a specific header\n",
    "uls = selector.xpath(\"//div[h4[contains(text(), 'By Age')]]/following-sibling::div//ul/li/text()\").getall()\n",
    "\n",
    "# Print extracted list items\n",
    "for line in uls:\n",
    "    print(line.strip())\n",
    "\n",
    "# Prepare data for Spark DataFrame\n",
    "data = []\n",
    "if paragraphsM:\n",
    "    data.append((\"adult men\", paragraphsM.strip()))\n",
    "if paragraphsF:\n",
    "    data.append((\"adult women\", paragraphsF.strip()))\n",
    "for line in uls:\n",
    "    data.append((\"By Age\", line.strip()))\n",
    "\n",
    "# Define schema\n",
    "columns = [\"Category\", \"Text\"]\n",
    "\n",
    "# Create a Spark DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc53cd5-f358-45bb-9f28-d26842b7a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define the function to get the smoking percentage based on age\n",
    "def get_smoking_percentage(age: int) -> float:\n",
    "    if 15 <= age <= 17:\n",
    "        return .016\n",
    "    elif 18 <= age <= 24:\n",
    "        return .073\n",
    "    elif 25 <= age <= 34:\n",
    "        return .109\n",
    "    elif 35 <= age <= 44:\n",
    "        return .109\n",
    "    elif 45 <= age <= 54:\n",
    "        return .138\n",
    "    elif 55 <= age <= 64:\n",
    "        return .149\n",
    "    elif 65 <= age <= 74:\n",
    "        return .087\n",
    "    elif age >= 75:\n",
    "        return .029\n",
    "    else:\n",
    "        return None  # Return None for unknown age ranges\n",
    "\n",
    "# Convert the function to a UDF\n",
    "get_smoking_udf = udf(lambda age: get_smoking_percentage(age), FloatType())\n",
    "\n",
    "# Convert the 'age' column to integer type\n",
    "df_subset = df_subset.withColumn('age', col('age').cast('int'))\n",
    "\n",
    "# Apply the function to create a new column 'smoking_src1' with the updated values\n",
    "df_subset = df_subset.withColumn('smoking_src1', \n",
    "                                 when(col('smoke').isin([0, 1]), col('smoke').cast(FloatType()))\n",
    "                                 .otherwise(get_smoking_udf(col('age'))))\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file, overwriting the existing file\n",
    "df_subset.write.csv('heart_disease_subset.csv', header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd22706-b165-4cbe-aa89-967bbfecef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Define the function to get the smoking percentage based on age and sex\n",
    "def get_smoking_percentage(age: int, sex: int) -> float:\n",
    "    if sex == 0:  # Female\n",
    "        if 18 <= age <= 24:\n",
    "            return .053\n",
    "        elif 25 <= age <= 44:\n",
    "            return .126\n",
    "        elif 45 <= age <= 64:\n",
    "            return .149\n",
    "        elif age >= 65:\n",
    "            return .083\n",
    "    elif sex == 1:  # Male\n",
    "        if 18 <= age <= 24:\n",
    "            return round(.053 * (.131 / .101), 3)\n",
    "        elif 25 <= age <= 44:\n",
    "            return round(.126 * (.131 / .101), 3)\n",
    "        elif 45 <= age <= 64:\n",
    "            return round(.149 * (.131 / .101), 3)\n",
    "        elif age >= 65:\n",
    "            return round(.083 * (.131 / .101), 3)\n",
    "    return None\n",
    "\n",
    "# Convert the function to a UDF\n",
    "get_smoking_udf = udf(lambda age, sex: get_smoking_percentage(age, sex), FloatType())\n",
    "\n",
    "# Convert the 'age' and 'sex' columns to integer type\n",
    "df_subset = df_subset.withColumn('age', col('age').cast('int'))\n",
    "df_subset = df_subset.withColumn('sex', col('sex').cast('int'))\n",
    "\n",
    "# Apply the function to create a new column 'smoke_src2' with the updated values\n",
    "df_subset = df_subset.withColumn('smoke_src2', \n",
    "                                 when(col('smoke').isin([0, 1]), col('smoke').cast(FloatType()))\n",
    "                                 .otherwise(get_smoking_udf(col('age'), col('sex'))))\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file, overwriting the existing file\n",
    "df_subset.write.csv('heart_disease_subset.csv', header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d4a6f4-8618-4700-bd59-5d773837fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'smoke' column\n",
    "df_subset = df_subset.drop('smoke')\n",
    "\n",
    "# Save the modified DataFrame to the same CSV file, overwriting the existing file\n",
    "df_subset.write.csv('heart_disease_subset.csv', header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "264e93d9-1842-4988-9eac-9aacbce7c73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 0.7500\n",
      "Random Forest Model Accuracy: 0.8056\n",
      "SVM Model Accuracy: 0.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/24 16:52:27 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "24/05/24 16:52:32 WARN BlockManager: Asked to remove block broadcast_3638, which does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Mean Cross-Validation Score: 0.8025\n",
      "Random Forest Model Mean Cross-Validation Score: 0.8062\n",
      "SVM Model Mean Cross-Validation Score: 0.8011\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv('heart_disease_subset.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Define the feature columns\n",
    "feature_cols = df.columns\n",
    "feature_cols.remove('target')\n",
    "\n",
    "# Create separate instances of VectorAssembler for each pipeline\n",
    "logistic_assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_logistic')\n",
    "random_forest_assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_random_forest')\n",
    "svm_assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_svm')\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_df, test_df = df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# Initialize the models\n",
    "logistic_model = LogisticRegression(labelCol='target', featuresCol='features_logistic', maxIter=1000)\n",
    "random_forest_model = RandomForestClassifier(labelCol='target', featuresCol='features_random_forest', maxDepth=10)\n",
    "svm_model = LinearSVC(labelCol='target', featuresCol='features_svm', maxIter=1000)\n",
    "\n",
    "# Create pipelines for each model\n",
    "logistic_pipeline = Pipeline(stages=[logistic_assembler, logistic_model])\n",
    "random_forest_pipeline = Pipeline(stages=[random_forest_assembler, random_forest_model])\n",
    "svm_pipeline = Pipeline(stages=[svm_assembler, svm_model])\n",
    "\n",
    "# Train the models\n",
    "logistic_model = logistic_pipeline.fit(train_df)\n",
    "random_forest_model = random_forest_pipeline.fit(train_df)\n",
    "svm_model = svm_pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "logistic_pred = logistic_model.transform(test_df)\n",
    "random_forest_pred = random_forest_model.transform(test_df)\n",
    "svm_pred = svm_model.transform(test_df)\n",
    "\n",
    "# Evaluate the performance of the models\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='target', metricName='accuracy')\n",
    "\n",
    "logistic_accuracy = evaluator.evaluate(logistic_pred)\n",
    "random_forest_accuracy = evaluator.evaluate(random_forest_pred)\n",
    "svm_accuracy = evaluator.evaluate(svm_pred)\n",
    "\n",
    "# Print the accuracy of each model\n",
    "print(f\"Logistic Regression Model Accuracy: {logistic_accuracy:.4f}\")\n",
    "print(f\"Random Forest Model Accuracy: {random_forest_accuracy:.4f}\")\n",
    "print(f\"SVM Model Accuracy: {svm_accuracy:.4f}\")\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval = CrossValidator(estimator=logistic_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cv_logistic_model = crossval.fit(train_df)\n",
    "logistic_cv_score = cv_logistic_model.avgMetrics[0]\n",
    "\n",
    "crossval = CrossValidator(estimator=random_forest_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cv_random_forest_model = crossval.fit(train_df)\n",
    "random_forest_cv_score = cv_random_forest_model.avgMetrics[0]\n",
    "\n",
    "crossval = CrossValidator(estimator=svm_pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "cv_svm_model = crossval.fit(train_df)\n",
    "svm_cv_score = cv_svm_model.avgMetrics[0]\n",
    "\n",
    "# Print mean cross-validation scores\n",
    "print(f\"Logistic Regression Model Mean Cross-Validation Score: {logistic_cv_score:.4f}\")\n",
    "print(f\"Random Forest Model Mean Cross-Validation Score: {random_forest_cv_score:.4f}\")\n",
    "print(f\"SVM Model Mean Cross-Validation Score: {svm_cv_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9a1b4-6b94-472a-bba4-d37f0ed916b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANALYSIS\n",
    "# Based on the results, I would choose the random forest model again. \n",
    "# This is an easier decision than last time, since now the random forest model has both a higher accuracy and a higher mean cross-validation score than\n",
    "# the other two models. This means that the random forest model performed better on the test data and is more likely to do well when testing on new data. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
